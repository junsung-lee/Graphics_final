{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Raw Data\n",
    "\n",
    "* This dataset previously contained information about user demographics. We are dropping these columns as we do not utilize this information for our visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./data/tweetdata_2011.csv\"\n",
    "\n",
    "data = pd.read_csv(path, sep=\"\\t\")\n",
    "#data.drop(data[\"american_indian\"])\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[\"american_indian\",\"asian\",\"black\",\"other\",\"pacific_islander\",\"white\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"lol\", \"fam\", \"swag\", \"yolo\", \"noob\", \"pwned\", \"epic fail\"]\n",
    "\n",
    "tweets_2011 = list(data['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict_2011 = {}\n",
    "for tweet in tweets_2011:\n",
    "    tweet = str(tweet)\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "#         print(word)\n",
    "        if(word not in tweet_dict_2011.keys()):\n",
    "            tweet_dict_2011[word] = 0\n",
    "        else:\n",
    "            tweet_dict_2011[word] += 1\n",
    "\n",
    "for word in tweet_dict_2011.keys():\n",
    "    if(tweet_dict_2011[word] >= 100):\n",
    "        print(word, \":\", tweet_dict_2011[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tweet_dict_2011.keys():\n",
    "    if(word.startswith(\"HASH\") and tweet_dict_2011[word] >= 75):\n",
    "        print(word, \":\", tweet_dict_2011[word])\n",
    "\n",
    "print(\"HASHsorrynotsorry\",\":\",tweet_dict_2011[\"HASHsorrynotsorry\"])\n",
    "print(\"HASHegypt\",\":\",tweet_dict_2011[\"HASHegypt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/clean_tweets2011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_file(path):\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(path, sep=\"\\t\")\n",
    "    #data.drop(data[\"american_indian\"])\n",
    "    print(data.head(5))\n",
    "    \n",
    "    data = data.drop(columns=[\"american_indian\",\"asian\",\"black\",\"other\",\"pacific_islander\",\"white\"])\n",
    "    print(data.head(10))\n",
    "    return data\n",
    "    \n",
    "\n",
    "path = \"./data/tweetdata_2012.csv\"\n",
    "new_file_name = \"./data/clean_tweets2012.csv\"\n",
    "\n",
    "data = clean_tweet_file(path)\n",
    "data.to_csv(new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2012 = list(data2[\"tweet_text\"])\n",
    "tweets_2012[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_2011.count(\"holy shit\"))\n",
    "print(tweets_2012.count(\"holy shit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict_2012 = {}\n",
    "for tweet in tweets_2012:\n",
    "    tweet = str(tweet)\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "#         print(word)\n",
    "        if(word not in tweet_dict_2012.keys()):\n",
    "            tweet_dict_2012[word] = 0\n",
    "        else:\n",
    "            tweet_dict_2012[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tweet_dict_2012.keys():\n",
    "    if(word.startswith(\"HASH\") and tweet_dict_2012[word] >= 500):\n",
    "        print(word, tweet_dict_2012[word])\n",
    "#     if(tweet_dict_2012[word] >= 100):\n",
    "#         print(word, tweet_dict_2012[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tweet_dict_2012.keys():\n",
    "    if(tweet_dict_2012[word] >= 100):\n",
    "        print(word, tweet_dict_2012[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the cleaned .csv files\n",
    "* We separated the tweets into three categories: vulgarities, words_of_interest, hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulgarities = [\"shit\",\"damn\",\"ass\",\"bitch\",\"fuck\",\"fucking\",\"fucked\",\"bullshit\",\"hell\"]\n",
    "words_of_interest = [\"stfu\",\"smh\",\"asap\",\"chillin\",\"wat\",\"xd\",\"af\",\"ugh\",\"sis\",\"wtf\"]\n",
    "hashtags = [\"HASHgetalljobs\",\"HASHjobs\",\"HASHsorrynotsorry\",\"HASHwinning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2011:\n",
    "vulgarities_2011,slang_2011,hashtags_2011 = {},{},{}\n",
    "for word in tweet_dict_2011.keys():\n",
    "    if word in vulgarities:\n",
    "        vulgarities_2011.update({word:tweet_dict_2011[word]})\n",
    "    elif word in words_of_interest:\n",
    "        slang_2011.update({word:tweet_dict_2011[word]})\n",
    "    elif word in hashtags:\n",
    "        hashtags_2011.update({word:tweet_dict_2011[word]})\n",
    "\n",
    "# for 2012:     \n",
    "vulgarities_2012,slang_2012,hashtags_2012 = {},{},{}\n",
    "for word in tweet_dict_2012.keys():\n",
    "    if word in vulgarities:\n",
    "        vulgarities_2012.update({word:tweet_dict_2012[word]})\n",
    "    elif word in words_of_interest:\n",
    "        slang_2012.update({word:tweet_dict_2012[word]})\n",
    "    elif word in hashtags:\n",
    "        hashtags_2012.update({word:tweet_dict_2012[word]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulgarities_df = [vulgarities_2011, vulgarities_2012]\n",
    "vulgarities_df = pd.DataFrame(vulgarities_df, index=['2011','2012']).T\n",
    "\n",
    "slang_df = [slang_2011,slang_2012]\n",
    "slang_df = pd.DataFrame(slang_df, index=['2011','2012']).T\n",
    "\n",
    "hashtags_df = [hashtags_2011, hashtags_2012]\n",
    "hashtags_df = pd.DataFrame(hashtags_df, index=['2011','2012']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulgarities_df.to_csv(\"/Users/sarikasabnis/Documents/GitHub/Graphics_final/final_project/data/vulgarity_counts.csv\")\n",
    "slang_df.to_csv(\"/Users/sarikasabnis/Documents/GitHub/Graphics_final/final_project/data/slang_counts.csv\")\n",
    "hashtags_df.to_csv(\"/Users/sarikasabnis/Documents/GitHub/Graphics_final/final_project/data/hashtag_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
